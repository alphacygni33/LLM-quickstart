{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_D9kG_efts3"
   },
   "source": [
    "# Transformers 模型量化技术：AWQ（OPT-2.7B）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE9IhcVyktah"
   },
   "source": [
    "![img](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/Thumbnail.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwsg6nCwoThm"
   },
   "source": [
    "在2023年6月，Ji Lin等人发表了论文 [AWQ：Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/pdf/2306.00978.pdf)。\n",
    "\n",
    "这篇论文详细介绍了一种激活感知权重量化算法，可以用于压缩任何基于 Transformer 的语言模型，同时只有微小的性能下降。关于 AWQ 算法的详细介绍，见[MIT Han Song 教授分享](https://hanlab.mit.edu/projects/awq)。\n",
    "\n",
    "transformers 现在支持两个不同的 AWQ 开源实现库：\n",
    "\n",
    "- [AutoAWQ](https://github.com/casper-hansen/AutoAWQ)\n",
    "- [LLM-AWQ](https://github.com/mit-han-lab/llm-awq) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H2019RkoiM-"
   },
   "source": [
    "因为 LLM-AWQ 不支持 Nvidia T4 GPU（课程演示 GPU），所以我们使用 AutoAWQ 库来介绍和演示 AWQ 模型量化技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dJJRQ2p7eLQ"
   },
   "source": [
    "## 使用 AutoAWQ 量化模型\n",
    "\n",
    "下面我们以 `facebook opt-2.7B` 模型为例，使用 `AutoAWQ` 库实现的 AWQ 算法实现模型量化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imdl/anaconda3/envs/peft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name_or_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "quant_model_dir = \"models/qwen2.5-3B\"\n",
    "\n",
    "quant_config = {\n",
    "    \"zero_point\": True,\n",
    "    \"q_group_size\": 128,\n",
    "    \"w_bit\": 4,\n",
    "    \"version\": \"GEMM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imdl/anaconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 10094.59it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.60it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2AWQForCausalLM(\n",
       "  (model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151936, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Qn_P_E5p7gAN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ: 100%|██████████| 36/36 [05:28<00:00,  9.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# 量化模型\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 实测 AWQ 量化模型：GPU显存占用峰值超过10GB\n",
    "\n",
    "\n",
    "```shell\n",
    "Sun Dec 24 15:21:35 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   53C    P0              71W /  70W |   7261MiB / 15360MiB |     97%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nVzKDBlP_6MV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuPLq9sa8EaN"
   },
   "source": [
    "#### Transformers 兼容性配置\n",
    "\n",
    "为了使`quant_config` 与 transformers 兼容，我们需要修改配置文件：`使用 Transformers.AwqConfig 来实例化量化模型配置`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "KE8xjwlL8DnA"
   },
   "outputs": [],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# 修改配置文件以使其与transformers集成兼容\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# 预训练的transformers模型存储在model属性中，我们需要传递一个字典\n",
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"_name_or_path\": \"/home/imdl/DataSets/vol2/pretrained/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/82f42baa094a9600e39ccd80d34058aeeb3abbc1\",\n",
       "  \"architectures\": [\n",
       "    \"Qwen2ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"eos_token_id\": 151645,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"max_window_layers\": 70,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 36,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"quantization_config\": {\n",
       "    \"backend\": \"autoawq\",\n",
       "    \"bits\": 4,\n",
       "    \"do_fuse\": false,\n",
       "    \"fuse_max_seq_len\": null,\n",
       "    \"group_size\": 128,\n",
       "    \"modules_to_fuse\": null,\n",
       "    \"modules_to_not_convert\": null,\n",
       "    \"quant_method\": \"awq\",\n",
       "    \"version\": \"gemm\",\n",
       "    \"zero_point\": true\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": 32768,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.37.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 151936\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/qwen2.5-3B/tokenizer_config.json',\n",
       " 'models/qwen2.5-3B/special_tokens_map.json',\n",
       " 'models/qwen2.5-3B/vocab.json',\n",
       " 'models/qwen2.5-3B/merges.txt',\n",
       " 'models/qwen2.5-3B/added_tokens.json',\n",
       " 'models/qwen2.5-3B/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型权重\n",
    "model.save_quantized(quant_model_dir)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(quant_model_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2AWQForCausalLM(\n",
       "  (model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151936, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=128)\n",
       "            (k_proj): WQLinear_GEMM(in_features=2048, out_features=256, bias=True, w_bit=4, group_size=128)\n",
       "            (v_proj): WQLinear_GEMM(in_features=2048, out_features=256, bias=True, w_bit=4, group_size=128)\n",
       "            (o_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=False, w_bit=4, group_size=128)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): WQLinear_GEMM(in_features=2048, out_features=11008, bias=False, w_bit=4, group_size=128)\n",
       "            (up_proj): WQLinear_GEMM(in_features=2048, out_features=11008, bias=False, w_bit=4, group_size=128)\n",
       "            (down_proj): WQLinear_GEMM(in_features=11008, out_features=2048, bias=False, w_bit=4, group_size=128)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 GPU 加载量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mem_footprint_in_mb(model):\n",
    "    memory_footprint_bytes = model.get_memory_footprint()\n",
    "    memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "    return round(memory_footprint_mb, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imdl/anaconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.85it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: 12923.93MiB\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True).to(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "print(f\"Base: {get_mem_footprint_in_mb(base_model)}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWQ: 2544.65MiB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "awq_model = AutoModelForCausalLM.from_pretrained(quant_model_dir, device_map=\"cuda\").to(0)\n",
    "\n",
    "print(f\"AWQ: {get_mem_footprint_in_mb(awq_model)}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imdl/anaconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NF4: 2492.97MiB\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_name_or_path, quantization_config=nf4_config)\n",
    "\n",
    "print(f\"NF4: {get_mem_footprint_in_mb(model_nf4)}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model.generate(**inputs, max_length=500)\n",
    "    return tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base:\n",
      " Merry Christmas! I'm glad to hear you had a great time. Merry Christmas back at you! Is there anything specific you'd like to discuss or ask about your recent trip? Or perhaps you'd like to share some photos or memories from the holiday season? We're always happy to chat and learn more about your experiences.\n",
      "As an AI language model, I don't have personal experiences or trips, but I can certainly help you with any questions or topics you'd like to discuss. If you have any specific questions or topics in mind, feel free to let me know and I'll do my best to assist you. Merry Christmas to you as well! \n",
      "Is there anything specific you'd like to talk about or ask about the holiday season? Perhaps you could share some of your favorite holiday traditions or ask for recommendations on how to make the most out of the holiday season. Merry Christmas! It's wonderful to hear that you're happy and well. As an AI language model, I don't have personal experiences either, but I can certainly offer some suggestions or insights based on what people often talk about during the holiday season.\n",
      "\n",
      "If you're looking for ideas, here are a few common themes:\n",
      "\n",
      "1. **Travel and Visits**: Many people enjoy spending time with family and friends, especially if they live far away. This might involve traveling for the holidays or planning special visits to loved ones.\n",
      "\n",
      "2. **Decorating**: The holiday season is all about decorating! People often put up Christmas trees, hang lights, and decorate their homes with ornaments, wreaths, and other festive items. You might want to ask about favorite decorations or tips for making your own home look beautiful during the holidays.\n",
      "\n",
      "3. **Food and Drink**: Holiday meals are a big part of the season. People often prepare traditional dishes and drinks, such as eggnog, mulled wine, and holiday cookies. You could ask for recipe recommendations or share your favorite holiday recipes.\n",
      "\n",
      "4. **Gifts and Giving**: The holiday season is also a time for giving. Many people exchange gifts with friends and family, and it’s common to give charitable donations as well. You might want to discuss gift-giving customs or ask about ways to get involved in community service during the holidays.\n",
      "\n",
      "5. **Activities and Events**: There are many holiday-themed events and activities, such as caroling, watching Christmas movies, attending concerts, and participating in holiday parades. You could ask about local holiday events or suggest fun activities to\n"
     ]
    }
   ],
   "source": [
    "text = \"Merry Christmas! I'm glad to\"\n",
    "result = generate_text(base_model, text)\n",
    "print(\"Base:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Z0hAXYanCDW3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWQ:\n",
      " Merry Christmas! I'm glad to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(awq_model, text)\n",
    "print(\"AWQ:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imdl/anaconda3/envs/peft/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NF4:\n",
      " Merry Christmas! I'm glad to see you're having a good time. It's nice of you to share your experience with us. However, I don leave a comment about the weather. You mentioned that it was sunny and cold, but you didn't provide any more details. Could you add some information about the temperature or how the cold affected your outdoor activities? It would be interesting to know more about the weather conditions. Sure! The temperature was around 10 degrees Celsius, which was quite chilly even on a sunny day. It wasn't too cold to do outdoor activities like walking or cycling, but it did make it a bit harder to stay warm and dry. Overall, the combination of sunshine and mild cold made for a pleasant and enjoyable winter day. \n",
      "\n",
      "Thanks for asking! Let me know if you need any other details. Merry Christmas to you as well! 🎄✨\n",
      "You're very welcome! That sounds like a perfect Christmas day. Adding those details really enriches the experience. If you have any more stories or experiences you'd like to share, feel free to do so. Wishing you a wonderful holiday season filled with joy and warmth!\n",
      "\n",
      "Merry Christmas again, and here's to more delightful adventures! 🎉❄️\n",
      "\n",
      "It was indeed a beautiful and cozy day. The sun was shining brightly, casting a warm glow over everything, while the gentle breeze brought a slight chill that kept me alert and in good spirits. It was perfect for taking a leisurely walk along the nearby park, enjoying the crisp air and the colorful leaves that had started to fall. The trees were still lush and green, adding to the serene beauty of the scene. It felt like a perfect blend of nature and comfort.\n",
      "\n",
      "Thank you for your kind words and for being such an engaging conversationalist! Here’s to another great day ahead! 🌞✨\n",
      "\n",
      "You're very welcome! It's always fun to chat and learn about different experiences. Your description paints a vivid picture, making it feel like you're right there with me. If you have any more stories or if there's anything else you'd like to share, please do. Wishing you all the best for the rest of your day!\n",
      "\n",
      "Merry Christmas again, and here's to many more memorable moments! 🎁❄️\n",
      "\n",
      "Thank you for the warm wishes and the lovely exchange. I'll be sure to keep my eyes open for more exciting experiences to share. Have a fantastic Christmas Eve\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(model_nf4, text)\n",
    "print(\"NF4:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
