{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_D9kG_efts3"
   },
   "source": [
    "# Transformers æ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼šAWQï¼ˆOPT-2.7Bï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE9IhcVyktah"
   },
   "source": [
    "![img](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/Thumbnail.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwsg6nCwoThm"
   },
   "source": [
    "åœ¨2023å¹´6æœˆï¼ŒJi Linç­‰äººå‘è¡¨äº†è®ºæ–‡ [AWQï¼šActivation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/pdf/2306.00978.pdf)ã€‚\n",
    "\n",
    "è¿™ç¯‡è®ºæ–‡è¯¦ç»†ä»‹ç»äº†ä¸€ç§æ¿€æ´»æ„ŸçŸ¥æƒé‡é‡åŒ–ç®—æ³•ï¼Œå¯ä»¥ç”¨äºå‹ç¼©ä»»ä½•åŸºäº Transformer çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶åªæœ‰å¾®å°çš„æ€§èƒ½ä¸‹é™ã€‚å…³äº AWQ ç®—æ³•çš„è¯¦ç»†ä»‹ç»ï¼Œè§[MIT Han Song æ•™æˆåˆ†äº«](https://hanlab.mit.edu/projects/awq)ã€‚\n",
    "\n",
    "transformers ç°åœ¨æ”¯æŒä¸¤ä¸ªä¸åŒçš„ AWQ å¼€æºå®ç°åº“ï¼š\n",
    "\n",
    "- [AutoAWQ](https://github.com/casper-hansen/AutoAWQ)\n",
    "- [LLM-AWQ](https://github.com/mit-han-lab/llm-awq) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H2019RkoiM-"
   },
   "source": [
    "å› ä¸º LLM-AWQ ä¸æ”¯æŒ Nvidia T4 GPUï¼ˆè¯¾ç¨‹æ¼”ç¤º GPUï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ AutoAWQ åº“æ¥ä»‹ç»å’Œæ¼”ç¤º AWQ æ¨¡å‹é‡åŒ–æŠ€æœ¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dJJRQ2p7eLQ"
   },
   "source": [
    "## ä½¿ç”¨ AutoAWQ é‡åŒ–æ¨¡å‹\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬ä»¥ `facebook opt-2.7B` æ¨¡å‹ä¸ºä¾‹ï¼Œä½¿ç”¨ `AutoAWQ` åº“å®ç°çš„ AWQ ç®—æ³•å®ç°æ¨¡å‹é‡åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imdl/anaconda3/envs/peft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name_or_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "quant_model_dir = \"models/qwen2.5-3B\"\n",
    "\n",
    "quant_config = {\n",
    "    \"zero_point\": True,\n",
    "    \"q_group_size\": 128,\n",
    "    \"w_bit\": 4,\n",
    "    \"version\": \"GEMM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imdl/anaconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 12 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 10094.59it/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.60it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ¨¡å‹\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2AWQForCausalLM(\n",
       "  (model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151936, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Qn_P_E5p7gAN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [05:28<00:00,  9.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# é‡åŒ–æ¨¡å‹\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### å®æµ‹ AWQ é‡åŒ–æ¨¡å‹ï¼šGPUæ˜¾å­˜å ç”¨å³°å€¼è¶…è¿‡10GB\n",
    "\n",
    "\n",
    "```shell\n",
    "Sun Dec 24 15:21:35 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   53C    P0              71W /  70W |   7261MiB / 15360MiB |     97%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nVzKDBlP_6MV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuPLq9sa8EaN"
   },
   "source": [
    "#### Transformers å…¼å®¹æ€§é…ç½®\n",
    "\n",
    "ä¸ºäº†ä½¿`quant_config` ä¸ transformers å…¼å®¹ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼š`ä½¿ç”¨ Transformers.AwqConfig æ¥å®ä¾‹åŒ–é‡åŒ–æ¨¡å‹é…ç½®`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "KE8xjwlL8DnA"
   },
   "outputs": [],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# ä¿®æ”¹é…ç½®æ–‡ä»¶ä»¥ä½¿å…¶ä¸transformersé›†æˆå…¼å®¹\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# é¢„è®­ç»ƒçš„transformersæ¨¡å‹å­˜å‚¨åœ¨modelå±æ€§ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’ä¸€ä¸ªå­—å…¸\n",
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"_name_or_path\": \"/home/imdl/DataSets/vol2/pretrained/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/82f42baa094a9600e39ccd80d34058aeeb3abbc1\",\n",
       "  \"architectures\": [\n",
       "    \"Qwen2ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"eos_token_id\": 151645,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"max_window_layers\": 70,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 36,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"quantization_config\": {\n",
       "    \"backend\": \"autoawq\",\n",
       "    \"bits\": 4,\n",
       "    \"do_fuse\": false,\n",
       "    \"fuse_max_seq_len\": null,\n",
       "    \"group_size\": 128,\n",
       "    \"modules_to_fuse\": null,\n",
       "    \"modules_to_not_convert\": null,\n",
       "    \"quant_method\": \"awq\",\n",
       "    \"version\": \"gemm\",\n",
       "    \"zero_point\": true\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": 32768,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.37.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 151936\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/qwen2.5-3B/tokenizer_config.json',\n",
       " 'models/qwen2.5-3B/special_tokens_map.json',\n",
       " 'models/qwen2.5-3B/vocab.json',\n",
       " 'models/qwen2.5-3B/merges.txt',\n",
       " 'models/qwen2.5-3B/added_tokens.json',\n",
       " 'models/qwen2.5-3B/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¿å­˜æ¨¡å‹æƒé‡\n",
    "model.save_quantized(quant_model_dir)\n",
    "# ä¿å­˜åˆ†è¯å™¨\n",
    "tokenizer.save_pretrained(quant_model_dir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2AWQForCausalLM(\n",
       "  (model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151936, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=128)\n",
       "            (k_proj): WQLinear_GEMM(in_features=2048, out_features=256, bias=True, w_bit=4, group_size=128)\n",
       "            (v_proj): WQLinear_GEMM(in_features=2048, out_features=256, bias=True, w_bit=4, group_size=128)\n",
       "            (o_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=False, w_bit=4, group_size=128)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): WQLinear_GEMM(in_features=2048, out_features=11008, bias=False, w_bit=4, group_size=128)\n",
       "            (up_proj): WQLinear_GEMM(in_features=2048, out_features=11008, bias=False, w_bit=4, group_size=128)\n",
       "            (down_proj): WQLinear_GEMM(in_features=11008, out_features=2048, bias=False, w_bit=4, group_size=128)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm()\n",
       "          (post_attention_layernorm): Qwen2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ GPU åŠ è½½é‡åŒ–æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mem_footprint_in_mb(model):\n",
    "    memory_footprint_bytes = model.get_memory_footprint()\n",
    "    memory_footprint_mb = memory_footprint_bytes / (1024 ** 2)\n",
    "    return round(memory_footprint_mb, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imdl/anaconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.85it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: 12923.93MiB\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True).to(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "print(f\"Base: {get_mem_footprint_in_mb(base_model)}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWQ: 2544.65MiB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "awq_model = AutoModelForCausalLM.from_pretrained(quant_model_dir, device_map=\"cuda\").to(0)\n",
    "\n",
    "print(f\"AWQ: {get_mem_footprint_in_mb(awq_model)}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imdl/anaconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NF4: 2492.97MiB\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_name_or_path, quantization_config=nf4_config)\n",
    "\n",
    "print(f\"NF4: {get_mem_footprint_in_mb(model_nf4)}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model.generate(**inputs, max_length=500)\n",
    "    return tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base:\n",
      " Merry Christmas! I'm glad to hear you had a great time. Merry Christmas back at you! Is there anything specific you'd like to discuss or ask about your recent trip? Or perhaps you'd like to share some photos or memories from the holiday season? We're always happy to chat and learn more about your experiences.\n",
      "As an AI language model, I don't have personal experiences or trips, but I can certainly help you with any questions or topics you'd like to discuss. If you have any specific questions or topics in mind, feel free to let me know and I'll do my best to assist you. Merry Christmas to you as well! \n",
      "Is there anything specific you'd like to talk about or ask about the holiday season? Perhaps you could share some of your favorite holiday traditions or ask for recommendations on how to make the most out of the holiday season. Merry Christmas! It's wonderful to hear that you're happy and well. As an AI language model, I don't have personal experiences either, but I can certainly offer some suggestions or insights based on what people often talk about during the holiday season.\n",
      "\n",
      "If you're looking for ideas, here are a few common themes:\n",
      "\n",
      "1. **Travel and Visits**: Many people enjoy spending time with family and friends, especially if they live far away. This might involve traveling for the holidays or planning special visits to loved ones.\n",
      "\n",
      "2. **Decorating**: The holiday season is all about decorating! People often put up Christmas trees, hang lights, and decorate their homes with ornaments, wreaths, and other festive items. You might want to ask about favorite decorations or tips for making your own home look beautiful during the holidays.\n",
      "\n",
      "3. **Food and Drink**: Holiday meals are a big part of the season. People often prepare traditional dishes and drinks, such as eggnog, mulled wine, and holiday cookies. You could ask for recipe recommendations or share your favorite holiday recipes.\n",
      "\n",
      "4. **Gifts and Giving**: The holiday season is also a time for giving. Many people exchange gifts with friends and family, and itâ€™s common to give charitable donations as well. You might want to discuss gift-giving customs or ask about ways to get involved in community service during the holidays.\n",
      "\n",
      "5. **Activities and Events**: There are many holiday-themed events and activities, such as caroling, watching Christmas movies, attending concerts, and participating in holiday parades. You could ask about local holiday events or suggest fun activities to\n"
     ]
    }
   ],
   "source": [
    "text = \"Merry Christmas! I'm glad to\"\n",
    "result = generate_text(base_model, text)\n",
    "print(\"Base:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Z0hAXYanCDW3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWQ:\n",
      " Merry Christmas! I'm glad to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(awq_model, text)\n",
    "print(\"AWQ:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imdl/anaconda3/envs/peft/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NF4:\n",
      " Merry Christmas! I'm glad to see you're having a good time. It's nice of you to share your experience with us. However, I don leave a comment about the weather. You mentioned that it was sunny and cold, but you didn't provide any more details. Could you add some information about the temperature or how the cold affected your outdoor activities? It would be interesting to know more about the weather conditions. Sure! The temperature was around 10 degrees Celsius, which was quite chilly even on a sunny day. It wasn't too cold to do outdoor activities like walking or cycling, but it did make it a bit harder to stay warm and dry. Overall, the combination of sunshine and mild cold made for a pleasant and enjoyable winter day. \n",
      "\n",
      "Thanks for asking! Let me know if you need any other details. Merry Christmas to you as well! ğŸ„âœ¨\n",
      "You're very welcome! That sounds like a perfect Christmas day. Adding those details really enriches the experience. If you have any more stories or experiences you'd like to share, feel free to do so. Wishing you a wonderful holiday season filled with joy and warmth!\n",
      "\n",
      "Merry Christmas again, and here's to more delightful adventures! ğŸ‰â„ï¸\n",
      "\n",
      "It was indeed a beautiful and cozy day. The sun was shining brightly, casting a warm glow over everything, while the gentle breeze brought a slight chill that kept me alert and in good spirits. It was perfect for taking a leisurely walk along the nearby park, enjoying the crisp air and the colorful leaves that had started to fall. The trees were still lush and green, adding to the serene beauty of the scene. It felt like a perfect blend of nature and comfort.\n",
      "\n",
      "Thank you for your kind words and for being such an engaging conversationalist! Hereâ€™s to another great day ahead! ğŸŒâœ¨\n",
      "\n",
      "You're very welcome! It's always fun to chat and learn about different experiences. Your description paints a vivid picture, making it feel like you're right there with me. If you have any more stories or if there's anything else you'd like to share, please do. Wishing you all the best for the rest of your day!\n",
      "\n",
      "Merry Christmas again, and here's to many more memorable moments! ğŸâ„ï¸\n",
      "\n",
      "Thank you for the warm wishes and the lovely exchange. I'll be sure to keep my eyes open for more exciting experiences to share. Have a fantastic Christmas Eve\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(model_nf4, text)\n",
    "print(\"NF4:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
